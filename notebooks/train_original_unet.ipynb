{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e919ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 1: Импорты\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from src.dataset_original import STEM4DDataset\n",
    "from src.model_original import U_Net\n",
    "from src.losses_original import CombinedLoss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Фиксируем seed для воспроизводимости\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad9dc819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Low dose shape: (256, 256, 48, 48)\n",
      "Low dose range: [0.00, 4.08]\n",
      "Average electrons per pattern: 165.4\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 2: Загрузка данных\n",
    "data_path = Path(\"../data\")\n",
    "results_path = Path(\"../results/original_unet\")\n",
    "results_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "low_dose = np.load(data_path / \"03_denoising_SrTiO3_High_mag_Low_dose.npy\")\n",
    "high_dose = np.load(data_path / \"03_denoising_SrTiO3_High_mag_High_dose.npy\")\n",
    "\n",
    "print(f\"Low dose shape: {low_dose.shape}\")\n",
    "print(f\"Low dose range: [{low_dose.min():.2f}, {low_dose.max():.2f}]\")\n",
    "\n",
    "# Вычисляем количество электронов на паттерн\n",
    "electrons_per_pattern = low_dose.sum(axis=(2, 3)).mean()\n",
    "print(f\"Average electrons per pattern: {electrons_per_pattern:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b2cf88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bright field pixels: 1812 / 2304\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ячейка 3: Создание bright field маски\n",
    "# PACBED из high dose для более точной маски\n",
    "pacbed_high = np.mean(high_dose, axis=(0, 1))\n",
    "threshold = 0.1 * pacbed_high.max()\n",
    "bf_mask = pacbed_high > threshold\n",
    "\n",
    "print(f\"Bright field pixels: {bf_mask.sum()} / {bf_mask.size}\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(pacbed_high, cmap='viridis')\n",
    "plt.title('PACBED')\n",
    "plt.subplot(122)\n",
    "plt.imshow(bf_mask, cmap='gray')\n",
    "plt.title('Bright Field Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d19125c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 64516\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 4: Создание Dataset и DataLoader\n",
    "batch_size = 8  # Как в оригинале\n",
    "window_size = 3  # 3x3 окно\n",
    "\n",
    "dataset = STEM4DDataset(\n",
    "    noisy_data=low_dose,\n",
    "    window_size=window_size,\n",
    "    bright_field_mask=bf_mask\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Разделение на train/val (90/10)\n",
    "n_samples = len(dataset)\n",
    "n_val = int(n_samples * 0.1)\n",
    "n_train = n_samples - n_val\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [n_train, n_val],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e50daf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 34,529,921\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 5: Инициализация модели\n",
    "n_kernels = 64  # Размер из статьи (38M параметров)\n",
    "model = U_Net(img_ch=8, output_ch=1, n_kernels=n_kernels).to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Loss с регуляризацией из статьи\n",
    "criterion = CombinedLoss(warmup_epochs=8, pacbed_weight=0.02, stem_weight=0.01)\n",
    "\n",
    "# Устанавливаем целевые значения для регуляризации\n",
    "target_pacbed = torch.from_numpy(bf_mask.astype(np.float32) * pacbed_high).to(device)\n",
    "target_stem_sums = torch.from_numpy(low_dose.flatten().astype(np.float32)).to(device)\n",
    "criterion.set_targets(target_pacbed, target_stem_sums)\n",
    "\n",
    "# Оптимизатор - SGD как упоминается в статье\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler для итеративного уточнения (умножение на 0.8)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f1a5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model to preserve total electron count...\n",
      "Initial scale factor: 256.6629\n",
      "Average input sum: 160.72\n",
      "Average output sum (before scaling): 0.00\n",
      "Average output sum (after scaling): 160.72\n"
     ]
    }
   ],
   "source": [
    "# Ячейка 6: Инициализация модели для сохранения общего количества электронов\n",
    "print(\"Initializing model to preserve total electron count...\")\n",
    "\n",
    "# Вычисляем начальные STEM и PACBED из данных\n",
    "noisy_stem = low_dose.sum(axis=(2, 3))\n",
    "noisy_pacbed = low_dose.sum(axis=(0, 1))\n",
    "\n",
    "# Делаем один проход для вычисления начального масштаба\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Берем небольшую выборку для оценки\n",
    "    sample_indices = np.random.choice(len(train_dataset), size=min(100, len(train_dataset)), replace=False)\n",
    "    \n",
    "    total_input_sum = 0\n",
    "    total_output_sum = 0\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        inputs, targets, _ = train_dataset[idx]\n",
    "        inputs = inputs.unsqueeze(0).to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Предсказание без масштабирования\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        total_input_sum += targets.sum().item()\n",
    "        total_output_sum += outputs.sum().item()\n",
    "    \n",
    "    # Вычисляем scale_factor\n",
    "    if total_output_sum > 0:\n",
    "        scale_factor = np.sqrt(total_input_sum / total_output_sum)\n",
    "    else:\n",
    "        scale_factor = 1.0\n",
    "    \n",
    "    # Устанавливаем в модель\n",
    "    model.scale_factor.fill_(scale_factor)\n",
    "    \n",
    "    print(f\"Initial scale factor: {scale_factor:.4f}\")\n",
    "    print(f\"Average input sum: {total_input_sum/len(sample_indices):.2f}\")\n",
    "    print(f\"Average output sum (before scaling): {total_output_sum/len(sample_indices):.2f}\")\n",
    "    print(f\"Average output sum (after scaling): {(total_output_sum * scale_factor**2)/len(sample_indices):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825bf0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 7: Функции обучения\n",
    "def train_epoch(model, loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    criterion.set_epoch(epoch)\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=f'Training epoch {epoch}')\n",
    "    for batch_idx, (inputs, targets, indices) in enumerate(pbar):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, indices)\n",
    "        \n",
    "        # Передаем indices в loss для STEM регуляризации\n",
    "        loss = criterion(outputs, targets, indices)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping для стабильности\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    criterion.set_epoch(epoch)\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, indices in tqdm(loader, desc='Validation'):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs, indices)\n",
    "            loss = criterion(outputs, targets, indices)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd7972b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with iterative refinement...\n",
      "Iterations: 10, Initial LR: 0.001\n",
      "\n",
      "=== Iteration 1/10 ===\n",
      "Learning rate: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 0:   7%|▋         | 531/7259 [11:16<2:22:45,  1.27s/it, loss=17238.7852]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs_per_iteration):\n\u001b[32m     20\u001b[39m     current_epoch = total_epochs + epoch\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     val_loss = validate_epoch(model, val_loader, criterion, device, \n\u001b[32m     25\u001b[39m                             current_epoch)\n\u001b[32m     27\u001b[39m     history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, epoch)\u001b[39m\n\u001b[32m     10\u001b[39m targets = targets.to(device)\n\u001b[32m     12\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Передаем indices в loss для STEM регуляризации\u001b[39;00m\n\u001b[32m     16\u001b[39m loss = criterion(outputs, targets, indices)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\Documents\\GitHub\\4dstem-denoising\\notebooks\\..\\src\\model_original.py:121\u001b[39m, in \u001b[36mU_Net.forward\u001b[39m\u001b[34m(self, x, inds)\u001b[39m\n\u001b[32m    118\u001b[39m d3 = torch.cat((x2, d3), dim=\u001b[32m1\u001b[39m)\n\u001b[32m    119\u001b[39m d3 = \u001b[38;5;28mself\u001b[39m.Up_conv3(d3)\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m d2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mUp2\u001b[49m\u001b[43m(\u001b[49m\u001b[43md3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m d2 = torch.cat((x1, d2), dim=\u001b[32m1\u001b[39m)\n\u001b[32m    123\u001b[39m d2 = \u001b[38;5;28mself\u001b[39m.Up_conv2(d2)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\Documents\\GitHub\\4dstem-denoising\\notebooks\\..\\src\\model_original.py:34\u001b[39m, in \u001b[36mup_conv.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\79152\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Ячейка 8: Основные параметры обучения из статьи\n",
    "num_iterations = 10  # 10 refinement steps как в статье\n",
    "epochs_per_iteration = 100  # Достаточно эпох для сходимости\n",
    "total_epochs = 0\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training with iterative refinement...\")\n",
    "print(f\"Iterations: {num_iterations}, Initial LR: {optimizer.param_groups[0]['lr']}\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"\\n=== Iteration {iteration + 1}/{num_iterations} ===\")\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    iteration_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs_per_iteration):\n",
    "        current_epoch = total_epochs + epoch\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, \n",
    "                               device, current_epoch)\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device, \n",
    "                                current_epoch)\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs_per_iteration} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Сохраняем лучшую модель\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss\n",
    "            }, results_path / 'best_model_original.pth')\n",
    "            print(f\"✓ Saved best model at epoch {current_epoch}\")\n",
    "    \n",
    "    total_epochs += epochs_per_iteration\n",
    "    \n",
    "    # Обновляем learning rate после каждой итерации (умножаем на 0.8)\n",
    "    scheduler.step()\n",
    "    \n",
    "    iteration_time = time.time() - iteration_start_time\n",
    "    print(f\"Iteration completed in {iteration_time/60:.2f} minutes\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal training time: {total_time/3600:.2f} hours\")\n",
    "\n",
    "# Визуализация истории обучения\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'], label='Train Loss', alpha=0.7)\n",
    "plt.plot(history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History (Original U-Net Implementation)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(results_path / 'training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999cb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 9: Применение коррекций STEM и PACBED\n",
    "print(\"\\nApplying STEM and PACBED corrections...\")\n",
    "\n",
    "# Загружаем лучшую модель\n",
    "checkpoint = torch.load(results_path / 'best_model_original.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Вычисляем коррекции как в оригинале\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Сначала получаем предсказания без коррекций\n",
    "    all_predictions = []\n",
    "    all_indices = []\n",
    "    \n",
    "    for inputs, targets, indices in tqdm(val_loader, desc='Computing corrections'):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs, indices)\n",
    "        all_predictions.append(outputs.cpu())\n",
    "        all_indices.extend(indices.numpy())\n",
    "    \n",
    "    predictions = torch.cat(all_predictions, dim=0)\n",
    "    \n",
    "    # Вычисляем предсказанные STEM и PACBED\n",
    "    pred_stem = predictions.sum(dim=(2, 3)).numpy()\n",
    "    pred_pacbed = predictions.sum(dim=0).squeeze().numpy()\n",
    "    \n",
    "    # Создаем коэффициенты коррекции\n",
    "    mu = np.ones(len(dataset))\n",
    "    for i, idx in enumerate(all_indices):\n",
    "        x, y = dataset.valid_positions[idx]\n",
    "        if pred_stem[i] > 0:\n",
    "            mu[idx] = noisy_stem[x, y] / pred_stem[i]\n",
    "    \n",
    "    pacbed_correction = np.ones_like(pred_pacbed)\n",
    "    pacbed_correction[pred_pacbed > 0] = noisy_pacbed[pred_pacbed > 0] / pred_pacbed[pred_pacbed > 0]\n",
    "    \n",
    "    # Устанавливаем коррекции в модель\n",
    "    model.mu = torch.from_numpy(mu).float().to(device)\n",
    "    model.PACBED = torch.from_numpy(pacbed_correction).float().to(device)\n",
    "\n",
    "print(\"Corrections applied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03762138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка 10: Функция полного деноизинга\n",
    "def denoise_full_dataset(model, noisy_data, bf_mask=None, batch_size=64, device='cpu'):\n",
    "    \"\"\"Применить деноизинг ко всему 4D STEM датасету\"\"\"\n",
    "    scan_x, scan_y, det_x, det_y = noisy_data.shape\n",
    "    denoised = np.zeros_like(noisy_data)\n",
    "    \n",
    "    # Создаем временный dataset\n",
    "    temp_dataset = STEM4DDataset(noisy_data, window_size=3, bright_field_mask=bf_mask)\n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_outputs = []\n",
    "        all_indices = []\n",
    "        \n",
    "        for inputs, targets, indices in tqdm(temp_loader, desc=\"Denoising\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs, indices)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_indices.extend(indices.numpy())\n",
    "        \n",
    "        # Размещаем результаты в правильные позиции\n",
    "        outputs = np.concatenate(all_outputs, axis=0)\n",
    "        for i, idx in enumerate(all_indices):\n",
    "            x, y = temp_dataset.valid_positions[idx]\n",
    "            denoised[x, y] = outputs[i, 0]\n",
    "    \n",
    "    # Копируем края (которые не были обработаны)\n",
    "    offset = 1\n",
    "    denoised[:offset, :] = noisy_data[:offset, :]\n",
    "    denoised[-offset:, :] = noisy_data[-offset:, :]\n",
    "    denoised[:, :offset] = noisy_data[:, :offset]\n",
    "    denoised[:, -offset:] = noisy_data[:, -offset:]\n",
    "    \n",
    "    return denoised\n",
    "\n",
    "# Применяем деноизинг\n",
    "print(\"\\nDenoising full dataset...\")\n",
    "denoised_data = denoise_full_dataset(model, low_dose, bf_mask, device=device)\n",
    "\n",
    "# Сохраняем результат\n",
    "np.save(results_path / 'denoised_original_unet.npy', denoised_data)\n",
    "\n",
    "# Визуализация результатов\n",
    "positions = [(50, 50), (100, 100), (150, 150)]\n",
    "fig, axes = plt.subplots(len(positions), 3, figsize=(12, 12))\n",
    "\n",
    "for i, (x, y) in enumerate(positions):\n",
    "    vmax = low_dose[x, y].max()\n",
    "    \n",
    "    axes[i, 0].imshow(low_dose[x, y], cmap='viridis', vmax=vmax)\n",
    "    axes[i, 0].set_title(f'Low Dose ({x}, {y})')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(denoised_data[x, y], cmap='viridis', vmax=vmax)\n",
    "    axes[i, 1].set_title('Denoised (Original U-Net)')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    axes[i, 2].imshow(high_dose[x, y], cmap='viridis')\n",
    "    axes[i, 2].set_title('High Dose (Reference)')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path / 'denoising_results.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDenoising completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
