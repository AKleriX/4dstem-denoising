{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e919ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from src.dataset_original import STEM4DDataset\n",
    "from src.model_original import U_Net\n",
    "from src.losses_original import CombinedLoss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Fix the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9dc819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Data download\n",
    "data_path = Path(\"../data\")\n",
    "results_path = Path(\"../results/original_unet\")\n",
    "results_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "low_dose = np.load(data_path / \"03_denoising_SrTiO3_High_mag_Low_dose.npy\")\n",
    "high_dose = np.load(data_path / \"03_denoising_SrTiO3_High_mag_High_dose.npy\")\n",
    "\n",
    "print(f\"Low dose shape: {low_dose.shape}\")\n",
    "print(f\"Low dose range: [{low_dose.min():.2f}, {low_dose.max():.2f}]\")\n",
    "\n",
    "# Calculating the number of electrons per pattern\n",
    "electrons_per_pattern = low_dose.sum(axis=(2, 3)).mean()\n",
    "print(f\"Average electrons per pattern: {electrons_per_pattern:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cf88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Creation of a bright field mask\n",
    "# PACBED from high dose for a more accurate mask\n",
    "pacbed_high = np.mean(high_dose, axis=(0, 1))\n",
    "threshold = 0.1 * pacbed_high.max()\n",
    "bf_mask = pacbed_high > threshold\n",
    "\n",
    "print(f\"Bright field pixels: {bf_mask.sum()} / {bf_mask.size}\")\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(121)\n",
    "plt.imshow(pacbed_high, cmap='viridis')\n",
    "plt.title('PACBED')\n",
    "plt.subplot(122)\n",
    "plt.imshow(bf_mask, cmap='gray')\n",
    "plt.title('Bright Field Mask')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d19125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Creating a Dataset and DataLoader\n",
    "batch_size = 8  \n",
    "window_size = 3  \n",
    "\n",
    "dataset = STEM4DDataset(\n",
    "    noisy_data=low_dose,\n",
    "    window_size=window_size,\n",
    "    bright_field_mask=bf_mask\n",
    ")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Division into train/val (90/10)\n",
    "n_samples = len(dataset)\n",
    "n_val = int(n_samples * 0.1)\n",
    "n_train = n_samples - n_val\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [n_train, n_val],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50daf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Model initialisation\n",
    "n_kernels = 12 \n",
    "model = U_Net(img_ch=8, output_ch=1, n_kernels=n_kernels).to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "criterion = CombinedLoss(warmup_epochs=8, pacbed_weight=0.02, stem_weight=0.01)\n",
    "\n",
    "target_pacbed = torch.from_numpy(bf_mask.astype(np.float32) * pacbed_high).to(device)\n",
    "target_stem_sums = torch.from_numpy(low_dose.flatten().astype(np.float32)).to(device)\n",
    "criterion.set_targets(target_pacbed, target_stem_sums)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Initialisation of the model to save the total number of electrons\n",
    "print(\"Initializing model to preserve total electron count...\")\n",
    "\n",
    "# Calculating initial STEM and PACBED from data\n",
    "noisy_stem = low_dose.sum(axis=(2, 3))\n",
    "noisy_pacbed = low_dose.sum(axis=(0, 1))\n",
    "\n",
    "# Make one pass to calculate the initial scale\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Take a small sample for evaluation\n",
    "    sample_indices = np.random.choice(len(train_dataset), size=min(100, len(train_dataset)), replace=False)\n",
    "    \n",
    "    total_input_sum = 0\n",
    "    total_output_sum = 0\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        inputs, targets, _ = train_dataset[idx]\n",
    "        inputs = inputs.unsqueeze(0).to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        total_input_sum += targets.sum().item()\n",
    "        total_output_sum += outputs.sum().item()\n",
    "    \n",
    "    \n",
    "    if total_output_sum > 0:\n",
    "        scale_factor = np.sqrt(total_input_sum / total_output_sum)\n",
    "    else:\n",
    "        scale_factor = 1.0\n",
    "    \n",
    "    \n",
    "    model.scale_factor.fill_(scale_factor)\n",
    "    \n",
    "    print(f\"Initial scale factor: {scale_factor:.4f}\")\n",
    "    print(f\"Average input sum: {total_input_sum/len(sample_indices):.2f}\")\n",
    "    print(f\"Average output sum (before scaling): {total_output_sum/len(sample_indices):.2f}\")\n",
    "    print(f\"Average output sum (after scaling): {(total_output_sum * scale_factor**2)/len(sample_indices):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bf0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Learning functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    criterion.set_epoch(epoch)\n",
    "    total_loss = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f'Training epoch {epoch}')\n",
    "    for batch_idx, (inputs, targets, indices) in enumerate(pbar):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, indices)\n",
    "\n",
    "        \n",
    "        loss = criterion(outputs, targets, indices)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    criterion.set_epoch(epoch)\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, indices in tqdm(loader, desc='Validation'):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs, indices)\n",
    "            loss = criterion(outputs, targets, indices)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7972b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Key learning parameters \n",
    "num_iterations = 4  \n",
    "epochs_per_iteration = 8  \n",
    "total_epochs = 0\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training with iterative refinement...\")\n",
    "print(f\"Iterations: {num_iterations}, Initial LR: {optimizer.param_groups[0]['lr']}\")\n",
    "total_start_time = time.time()\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    print(f\"\\n=== Iteration {iteration + 1}/{num_iterations} ===\")\n",
    "    print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "    iteration_start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs_per_iteration):\n",
    "        current_epoch = total_epochs + epoch\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer,\n",
    "                               device, current_epoch)\n",
    "        val_loss = validate_epoch(model, val_loader, criterion, device,\n",
    "                                current_epoch)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs_per_iteration} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Keeping the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'epoch': current_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss\n",
    "            }, results_path / 'best_model_original.pth')\n",
    "            print(f\"✓ Saved best model at epoch {current_epoch}\")\n",
    "\n",
    "    total_epochs += epochs_per_iteration\n",
    "\n",
    "    # Update the learning rate after each iteration (multiply by 0.8)\n",
    "    scheduler.step()\n",
    "\n",
    "    iteration_time = time.time() - iteration_start_time\n",
    "    print(f\"Iteration completed in {iteration_time/60:.2f} minutes\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal training time: {total_time/3600:.2f} hours\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history['train_loss'], label='Train Loss', alpha=0.7)\n",
    "plt.plot(history['val_loss'], label='Val Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training History (Original U-Net Implementation)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(results_path / 'training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999cb07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Applying STEM and PACBED corrections\n",
    "print(\"\\nApplying STEM and PACBED corrections...\")\n",
    "\n",
    "# Download the best model\n",
    "checkpoint = torch.load(results_path / 'best_model_original.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_indices = []\n",
    "\n",
    "    for inputs, targets, indices in tqdm(val_loader, desc='Computing corrections'):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs, indices)\n",
    "        all_predictions.append(outputs.cpu())\n",
    "        all_indices.extend(indices.numpy())\n",
    "\n",
    "    predictions = torch.cat(all_predictions, dim=0)\n",
    "\n",
    "    \n",
    "    pred_stem = predictions.sum(dim=(2, 3)).numpy()\n",
    "    pred_pacbed = predictions.sum(dim=0).squeeze().numpy()\n",
    "\n",
    "    \n",
    "    mu = np.ones(len(dataset))\n",
    "    for i, idx in enumerate(all_indices):\n",
    "        x, y = dataset.valid_positions[idx]\n",
    "        if pred_stem[i] > 0:\n",
    "            mu[idx] = noisy_stem[x, y] / pred_stem[i]\n",
    "\n",
    "    pacbed_correction = np.ones_like(pred_pacbed)\n",
    "    pacbed_correction[pred_pacbed > 0] = noisy_pacbed[pred_pacbed > 0] / pred_pacbed[pred_pacbed > 0]\n",
    "\n",
    "    \n",
    "    model.mu = torch.from_numpy(mu).float().to(device)\n",
    "    model.PACBED = torch.from_numpy(pacbed_correction).float().to(device)\n",
    "\n",
    "print(\"Corrections applied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03762138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Full denoising function\n",
    "def denoise_full_dataset(model, noisy_data, bf_mask=None, batch_size=64, device='cpu'):\n",
    "    \n",
    "    scan_x, scan_y, det_x, det_y = noisy_data.shape\n",
    "    denoised = np.zeros_like(noisy_data)\n",
    "\n",
    "    \n",
    "    temp_dataset = STEM4DDataset(noisy_data, window_size=3, bright_field_mask=bf_mask)\n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_outputs = []\n",
    "        all_indices = []\n",
    "\n",
    "        for inputs, targets, indices in tqdm(temp_loader, desc=\"Denoising\"):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs, indices)\n",
    "            all_outputs.append(outputs.cpu().numpy())\n",
    "            all_indices.extend(indices.numpy())\n",
    "\n",
    "        outputs = np.concatenate(all_outputs, axis=0)\n",
    "        for i, idx in enumerate(all_indices):\n",
    "            x, y = temp_dataset.valid_positions[idx]\n",
    "            denoised[x, y] = outputs[i, 0]\n",
    "\n",
    "    \n",
    "    offset = 1\n",
    "    denoised[:offset, :] = noisy_data[:offset, :]\n",
    "    denoised[-offset:, :] = noisy_data[-offset:, :]\n",
    "    denoised[:, :offset] = noisy_data[:, :offset]\n",
    "    denoised[:, -offset:] = noisy_data[:, -offset:]\n",
    "\n",
    "    return denoised\n",
    "\n",
    "\n",
    "print(\"\\nDenoising full dataset...\")\n",
    "denoised_data = denoise_full_dataset(model, low_dose, bf_mask, device=device)\n",
    "\n",
    "\n",
    "np.save(results_path / 'denoised_original_unet.npy', denoised_data)\n",
    "\n",
    "\n",
    "positions = [(10, 50), (100, 100), (150, 150)]\n",
    "fig, axes = plt.subplots(len(positions), 3, figsize=(12, 12))\n",
    "\n",
    "for i, (x, y) in enumerate(positions):\n",
    "    vmax = low_dose[x, y].max()\n",
    "\n",
    "    axes[i, 0].imshow(low_dose[x, y], cmap='viridis', vmax=vmax)\n",
    "    axes[i, 0].set_title(f'Low Dose ({x}, {y})')\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    axes[i, 1].imshow(denoised_data[x, y], cmap='viridis', vmax=vmax)\n",
    "    axes[i, 1].set_title('Denoised (Original U-Net)')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "    axes[i, 2].imshow(high_dose[x, y], cmap='viridis')\n",
    "    axes[i, 2].set_title('High Dose (Reference)')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_path / 'denoising_results.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDenoising completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
